{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Seed Points for Expansion\n",
        "\n",
        "Generate potential new store locations using the top 25% of H3 cells with highest total POI counts from the gold table.\n",
        "\n",
        "These locations represent areas with high commercial activity and could be good candidates for new store openings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read h3_features_gold table\n",
        "h3_features_df = spark.table(f\"{catalog}.gold.h3_features_gold\")\n",
        "\n",
        "# Calculate total POI count threshold for top 25%\n",
        "poi_percentile = h3_features_df.selectExpr(\"percentile_approx(total_poi_count, 0.75) as p75\").collect()[0]['p75']\n",
        "\n",
        "print(f\"Total POI count threshold (75th percentile): {poi_percentile}\")\n",
        "\n",
        "# Filter to top 25% by total POI count\n",
        "top_h3_cells = (h3_features_df\n",
        "                .filter(F.col(\"total_poi_count\") >= poi_percentile)\n",
        "                .filter(F.col(\"total_poi_count\") > 0)  # Ensure we have actual POIs\n",
        "                )\n",
        "\n",
        "print(f\"Number of H3 cells in top 25%: {top_h3_cells.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate centroids of H3 cells and get geographic context\n",
        "h3_with_centroids = (top_h3_cells\n",
        "    .withColumn(\"center_wkt\", F.expr(\"h3_centeraswkt(h3_cell_id)\"))\n",
        "    .withColumn(\"center_point\", F.expr(\"ST_GeomFromWKT(center_wkt, 4326)\"))\n",
        "    .withColumn(\"latitude\", F.expr(\"ST_Y(center_point)\"))\n",
        "    .withColumn(\"longitude\", F.expr(\"ST_X(center_point)\"))\n",
        ")\n",
        "\n",
        "# Load block groups to get city/state information via spatial join\n",
        "blockgroups_geo = spark.table(f\"{catalog}.bronze.census_blockgroups\") \\\n",
        "    .select(\"geoid\", \"name\", \"state_fips\", \"county_fips\", \"geometry\")\n",
        "\n",
        "# Spatial join to get geographic attributes\n",
        "h3_with_geo = h3_with_centroids.join(\n",
        "    F.broadcast(blockgroups_geo),\n",
        "    F.expr(\"ST_Contains(geometry, center_point)\"),\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "display(h3_with_geo.select(\"h3_cell_id\", \"latitude\", \"longitude\", \"name\", \"state_fips\", \"total_poi_count\").limit(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format seed points to match RMC locations structure\n",
        "# Add row number for store numbering\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window_spec = Window.orderBy(F.desc(\"total_poi_count\"))\n",
        "\n",
        "seed_points_expansion = (h3_with_geo\n",
        "    .withColumn(\"row_num\", F.row_number().over(window_spec))\n",
        "    .withColumn(\"store_number\", (20000 + F.col(\"row_num\")).cast(\"string\"))  # Start at 20001\n",
        "    .withColumn(\"store_type\", F.lit(\"New Expansion\"))\n",
        "    .withColumn(\"address\", F.concat(F.col(\"store_number\"), F.lit(\" Expansion Blvd\")))\n",
        "    .withColumn(\"city\", \n",
        "                F.when(F.col(\"name\").isNotNull(), \n",
        "                       F.regexp_extract(F.col(\"name\"), r\"^(.*?),\", 1))\n",
        "                .otherwise(\"Boston\"))  # Default to Boston if no match\n",
        "    .withColumn(\"state\", F.lit(\"MA\"))\n",
        "    .withColumn(\"zip_code\", F.lit(\"02101\"))  # Placeholder\n",
        "    .withColumn(\"phone_number\", F.concat(\n",
        "        F.lit(\"(774) \"),\n",
        "        F.lpad((800 + (F.col(\"row_num\") % 200)).cast(\"string\"), 3, \"0\"),\n",
        "        F.lit(\"-\"),\n",
        "        F.lpad((F.col(\"row_num\") * 17 % 10000).cast(\"string\"), 4, \"0\")\n",
        "    ))\n",
        "    .withColumn(\"store_hours\", F.lit(\"Monday: 08:00-22:00|Tuesday: 08:00-22:00|Wednesday: 08:00-22:00|Thursday: 08:00-22:00|Friday: 08:00-22:00|Saturday: 08:00-22:00|Sunday: 08:00-22:00\"))\n",
        "    .withColumn(\"store_services\", F.lit(\"Planned Location\"))\n",
        "    .withColumn(\"country\", F.lit(\"United States\"))\n",
        "    .withColumn(\"county\", \n",
        "                F.when(F.col(\"county_fips\").isNotNull(), F.col(\"county_fips\"))\n",
        "                .otherwise(\"Suffolk County\"))\n",
        "    .withColumn(\"country_code\", F.lit(\"US\"))\n",
        "    .withColumn(\"geo_accuracy\", F.lit(\"H3_CENTROID\"))\n",
        "    .select(\n",
        "        \"store_number\", \"store_type\", \"address\", \"city\", \"state\", \"zip_code\",\n",
        "        \"phone_number\", \"latitude\", \"longitude\", \"store_hours\", \"store_services\",\n",
        "        \"country\", \"county\", \"country_code\", \"geo_accuracy\"\n",
        "    )\n",
        ")\n",
        "\n",
        "display(seed_points_expansion.limit(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write seed points to bronze layer\n",
        "seed_points_table = f\"{catalog}.bronze.seed_points_expansion\"\n",
        "\n",
        "(seed_points_expansion\n",
        " .write\n",
        " .mode(\"overwrite\")\n",
        " .saveAsTable(seed_points_table))\n",
        "\n",
        "print(f\"Written seed points expansion to {seed_points_table}\")\n",
        "\n",
        "# Verify the structure matches RMC locations\n",
        "display(spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_seed_points,\n",
        "        COUNT(DISTINCT city) as unique_cities,\n",
        "        COUNT(DISTINCT store_type) as unique_store_types,\n",
        "        MIN(latitude) as min_lat,\n",
        "        MAX(latitude) as max_lat,\n",
        "        MIN(longitude) as min_lon,\n",
        "        MAX(longitude) as max_lon\n",
        "    FROM {seed_points_table}\n",
        "\"\"\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize seed points on map\n",
        "display(spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        store_number,\n",
        "        store_type,\n",
        "        city,\n",
        "        state,\n",
        "        latitude,\n",
        "        longitude,\n",
        "        store_services,\n",
        "        geo_accuracy\n",
        "    FROM {seed_points_table}\n",
        "    ORDER BY store_number\n",
        "\"\"\"))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (Databricks)",
      "language": "python",
      "name": "python3-databricks"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
