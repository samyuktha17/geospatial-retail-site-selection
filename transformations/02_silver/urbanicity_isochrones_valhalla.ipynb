{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "# MAGIC %md\n",
        "# MAGIC # Urbanicity-Based Routing with Valhalla\n",
        "# MAGIC\n",
        "# MAGIC Generates drive-time isochrones based on urbanicity classification.\n",
        "# MAGIC\n",
        "# MAGIC **Drive Times:** Urban=10min, Suburban=20min (default), Rural=30min"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md\n",
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install -q pyyaml h3"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dbutils.widgets.text(\"catalog\", \"geo_site_selection\")\n",
        "dbutils.widgets.text(\"bronze_schema\", \"bronze\")\n",
        "dbutils.widgets.text(\"silver_schema\", \"silver\")\n",
        "dbutils.widgets.text(\"gold_schema\", \"gold\")\n",
        "dbutils.widgets.text(\"config_path\", \"/Workspace/resources/configs/isochrone_config.yml\")\n",
        "dbutils.widgets.text(\"input_table\", \"\", \"Input Table (optional)\")\n",
        "dbutils.widgets.text(\"output_table_override\", \"\", \"Output Table (optional)\")\n",
        "dbutils.widgets.dropdown(\"skip_setup\", \"yes\", [\"yes\", \"no\"], \"Skip Valhalla Setup\")\n",
        "\n",
        "catalog = dbutils.widgets.get(\"catalog\")\n",
        "bronze_schema = dbutils.widgets.get(\"bronze_schema\")\n",
        "silver_schema = dbutils.widgets.get(\"silver_schema\")\n",
        "gold_schema = dbutils.widgets.get(\"gold_schema\")\n",
        "config_path = dbutils.widgets.get(\"config_path\")\n",
        "input_table_override = dbutils.widgets.get(\"input_table\")\n",
        "output_table_override = dbutils.widgets.get(\"output_table_override\")\n",
        "skip_setup = dbutils.widgets.get(\"skip_setup\") == \"yes\"\n",
        "\n",
        "BUILD_PATH = \"/local_disk0/valhalla_build\"\n",
        "VALHALLA_CONFIG = f\"{BUILD_PATH}/valhalla.json\"\n",
        "OSM_VOLUME = f\"/Volumes/{catalog}/{bronze_schema}/osm_data\"\n",
        "PERSIST_VOLUME = f\"/Volumes/{catalog}/{silver_schema}/valhalla_data\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "input_table_override"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import yaml\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "urbanicity_config = config['urbanicity_routing']\n",
        "perf_config = config['performance']\n",
        "output_config = config['output']\n",
        "\n",
        "h3_features_table = urbanicity_config['h3_features_table']\n",
        "drive_times = urbanicity_config['drive_times']\n",
        "repartition_factor = perf_config.get('repartition_factor', 8)\n",
        "\n",
        "# Use parameter override if provided, otherwise use config\n",
        "if input_table_override and input_table_override.strip():\n",
        "    locations_table = input_table_override.strip()\n",
        "else:\n",
        "    locations_table = config.get('isochrone', {}).get('input_tables', {}).get('rmc',\n",
        "        f\"{catalog}.{bronze_schema}.rmc_retail_locations_grocery\")\n",
        "\n",
        "if output_table_override and output_table_override.strip():\n",
        "    output_table = output_table_override.strip()\n",
        "else:\n",
        "    output_table = urbanicity_config['output_table']\n",
        "\n",
        "print(f\"Input: {locations_table}\")\n",
        "print(f\"Output: {catalog}.{silver_schema}.{output_table}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md\n",
        "## Setup Valhalla (One-Time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%sh\n",
        "if [ \"$skip_setup\" = \"yes\" ]; then exit 0; fi\n",
        "\n",
        "sudo apt-get update -y > /dev/null 2>&1\n",
        "sudo apt-get install -y cmake build-essential git curl wget > /dev/null 2>&1"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%sh\n",
        "if [ -n \"${skip_setup}\" ] && [ \"${skip_setup}\" != \"no\" ]; then exit 0; fi\n",
        "\n",
        "if command -v valhalla_build_config >/dev/null 2>&1 && python3 -c \"import valhalla\" 2>/dev/null; then\n",
        "    exit 0\n",
        "fi\n",
        "\n",
        "set -e\n",
        "BUILD_DIR=\"/local_disk0/tmp/valhalla_build\"\n",
        "rm -rf \"$BUILD_DIR\"\n",
        "mkdir -p \"$BUILD_DIR\"\n",
        "cd \"$BUILD_DIR\"\n",
        "\n",
        "git clone --quiet --recurse-submodules https://github.com/valhalla/valhalla.git\n",
        "cd valhalla\n",
        "\n",
        "sudo ./scripts/install-linux-deps.sh > /dev/null 2>&1\n",
        "\n",
        "cmake -B build \\\n",
        "  -DCMAKE_BUILD_TYPE=Release \\\n",
        "  -DENABLE_PYTHON_BINDINGS=ON \\\n",
        "  -DCMAKE_CXX_FLAGS=\"-Wno-error=format-truncation\" \\\n",
        "  > /dev/null 2>&1\n",
        "\n",
        "make -C build -j$(nproc) > /dev/null 2>&1\n",
        "sudo make -C build install > /dev/null 2>&1\n",
        "sudo ldconfig"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%sh\n",
        "if [ -n \"${skip_setup}\" ] && [ \"${skip_setup}\" != \"no\" ]; then exit 0; fi\n",
        "\n",
        "BUILD_DIR=\"/local_disk0/valhalla_build\"\n",
        "OSM_VOLUME=\"/Volumes/retail_consumer_goods/geospatial_site_selection/osm_data\"\n",
        "\n",
        "if [ -f \"$BUILD_DIR/valhalla_tiles.tar\" ]; then exit 0; fi\n",
        "\n",
        "mkdir -p \"$BUILD_DIR\"\n",
        "cd \"$BUILD_DIR\"\n",
        "\n",
        "PBF_FILE=$(ls \"$OSM_VOLUME\"/*.osm.pbf 2>/dev/null | head -1)\n",
        "if [ -z \"$PBF_FILE\" ]; then\n",
        "    echo \"No .osm.pbf file found in $OSM_VOLUME\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "mkdir -p valhalla_tiles\n",
        "valhalla_build_config \\\n",
        "  --mjolnir-tile-dir \"${BUILD_DIR}/valhalla_tiles\" \\\n",
        "  --mjolnir-tile-extract \"${BUILD_DIR}/valhalla_tiles.tar\" \\\n",
        "  --mjolnir-timezone \"${BUILD_DIR}/valhalla_tiles/timezones.sqlite\" \\\n",
        "  --mjolnir-admin \"${BUILD_DIR}/valhalla_tiles/admins.sqlite\" > valhalla.json\n",
        "\n",
        "valhalla_build_timezones > valhalla_tiles/timezones.sqlite\n",
        "valhalla_build_admins -c valhalla.json \"$PBF_FILE\"\n",
        "valhalla_build_tiles -c valhalla.json \"$PBF_FILE\"\n",
        "valhalla_build_extract -c valhalla.json -v"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil\n",
        "if not skip_setup:\n",
        "    import os\n",
        "    \n",
        "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{silver_schema}.silver_valhalla_data\")\n",
        "    \n",
        "    config_source = f\"{BUILD_PATH}/valhalla.json\"\n",
        "    config_dest = f\"{PERSIST_VOLUME}/valhalla.json\"\n",
        "    if os.path.exists(config_source):\n",
        "        shutil.copy(config_source, config_dest.replace(\"dbfs:\", \"/dbfs\"))\n",
        "    \n",
        "    tiles_source = f\"{BUILD_PATH}/valhalla_tiles.tar\"\n",
        "    tiles_dest = f\"{PERSIST_VOLUME}/valhalla_tiles.tar\"\n",
        "    if os.path.exists(tiles_source):\n",
        "        shutil.copy(tiles_source, tiles_dest.replace(\"dbfs:\", \"/dbfs\"))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import json\n",
        "import valhalla\n",
        "\n",
        "if not os.path.exists(VALHALLA_CONFIG):\n",
        "    raise FileNotFoundError(f\"Config not found: {VALHALLA_CONFIG}. Set skip_setup=no and rerun.\")\n",
        "\n",
        "actor = valhalla.Actor(VALHALLA_CONFIG)\n",
        "status_json = actor.status()\n",
        "status = json.loads(status_json) if isinstance(status_json, str) else status_json\n",
        "\n",
        "print(f\"Valhalla {status.get('version', 'unknown')} ready\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md\n",
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col, expr, coalesce, monotonically_increasing_id, broadcast, lit\n",
        "\n",
        "# Read table and auto-detect columns\n",
        "df = spark.read.table(locations_table)\n",
        "columns = df.columns\n",
        "\n",
        "# Flexible column mapping\n",
        "id_col = next((c for c in columns if c in ['store_number', 'point_id', 'id', 'location_id']), None)\n",
        "lat_col = next((c for c in columns if c in ['latitude', 'lat', 'y']), None)\n",
        "lon_col = next((c for c in columns if c in ['longitude', 'lon', 'lng', 'x']), None)\n",
        "type_col = next((c for c in columns if c in ['store_type', 'type', 'category']), None)\n",
        "city_col = next((c for c in columns if c in ['city', 'municipality']), None)\n",
        "state_col = next((c for c in columns if c in ['state', 'region']), None)\n",
        "\n",
        "if not lat_col or not lon_col:\n",
        "    raise ValueError(f\"Could not find latitude/longitude columns in {locations_table}. Available: {columns}\")\n",
        "\n",
        "# Build select with available columns\n",
        "select_cols = [\n",
        "    coalesce(col(id_col), monotonically_increasing_id().cast(\"string\")).alias(\"store_number\") if id_col else monotonically_increasing_id().cast(\"string\").alias(\"store_number\"),\n",
        "    col(lat_col).alias(\"latitude\"),\n",
        "    col(lon_col).alias(\"longitude\"),\n",
        "    col(type_col).alias(\"store_type\") if type_col else lit(None).alias(\"store_type\"),\n",
        "    col(city_col).alias(\"city\") if city_col else lit(None).alias(\"city\"),\n",
        "    col(state_col).alias(\"state\") if state_col else lit(None).alias(\"state\")\n",
        "]\n",
        "\n",
        "locations = df.select(*select_cols).filter(col(\"latitude\").isNotNull() & col(\"longitude\").isNotNull())\n",
        "\n",
        "location_count = locations.count()\n",
        "print(f\"{location_count} locations\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "h3_features = (spark.read.table(f'{catalog}.{gold_schema}.silver_h3_features')\n",
        "    .select(\n",
        "        col(\"h3_cell_id\"),\n",
        "        col(\"urbanicity_category\"),\n",
        "        col(\"urbanicity_score\")\n",
        "    )\n",
        ")\n",
        "\n",
        "locations_with_h3 = locations.withColumn(\n",
        "    \"h3_cell_id\",\n",
        "    expr(\"h3_longlatash3string(longitude, latitude, 8)\")\n",
        ")\n",
        "\n",
        "locations_with_urbanicity = locations_with_h3.join(\n",
        "    broadcast(h3_features),\n",
        "    \"h3_cell_id\",\n",
        "    \"left\"\n",
        ").fillna(\n",
        "    {\"urbanicity_category\": \"suburban\", \"urbanicity_score\": 0.0}\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "locations_with_drive_time = locations_with_urbanicity.withColumn(\n",
        "    \"drive_time_minutes\",\n",
        "    expr(f\"\"\"\n",
        "        CASE\n",
        "            WHEN urbanicity_category = 'urban' THEN {drive_times['urban']}\n",
        "            WHEN urbanicity_category = 'suburban' THEN {drive_times['suburban']}\n",
        "            WHEN urbanicity_category = 'rural' THEN {drive_times['rural']}\n",
        "            ELSE {drive_times['suburban']}\n",
        "        END\n",
        "    \"\"\")\n",
        ")\n",
        "\n",
        "display(\n",
        "    locations_with_drive_time\n",
        "    .groupBy(\"urbanicity_category\", \"drive_time_minutes\")\n",
        "    .count()\n",
        "    .orderBy(\"urbanicity_category\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(locations_with_drive_time.limit(5))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md\n",
        "## Generate Isochrones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
        "\n",
        "def geojson_to_wkt(geojson_geom):\n",
        "    \"\"\"Convert GeoJSON geometry to WKT\"\"\"\n",
        "    geom_type = geojson_geom.get('type')\n",
        "    coords = geojson_geom.get('coordinates', [])\n",
        "    \n",
        "    if geom_type == 'Polygon':\n",
        "        rings = []\n",
        "        for ring in coords:\n",
        "            ring_coords = ', '.join([f\"{lon} {lat}\" for lon, lat in ring])\n",
        "            rings.append(f\"({ring_coords})\")\n",
        "        return f\"POLYGON ({', '.join(rings)})\"\n",
        "    \n",
        "    elif geom_type == 'MultiPolygon':\n",
        "        polygons = []\n",
        "        for polygon in coords:\n",
        "            rings = []\n",
        "            for ring in polygon:\n",
        "                ring_coords = ', '.join([f\"{lon} {lat}\" for lon, lat in ring])\n",
        "                rings.append(f\"({ring_coords})\")\n",
        "            polygons.append(f\"({', '.join(rings)})\")\n",
        "        return f\"MULTIPOLYGON ({', '.join(polygons)})\"\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported geometry type: {geom_type}\")\n",
        "\n",
        "def generate_isochrone(row):\n",
        "    try:\n",
        "        query = {\n",
        "            \"locations\": [{\"lat\": float(row.latitude), \"lon\": float(row.longitude)}],\n",
        "            \"costing\": \"auto\",\n",
        "            \"contours\": [{\"time\": float(row.drive_time_minutes)}],\n",
        "            \"polygons\": True\n",
        "        }\n",
        "        \n",
        "        result_json = actor.isochrone(json.dumps(query))\n",
        "        result = json.loads(result_json) if isinstance(result_json, str) else result_json\n",
        "        \n",
        "        if result and 'features' in result and len(result['features']) > 0:\n",
        "            feature = result['features'][0]\n",
        "            geometry = feature.get('geometry')\n",
        "            \n",
        "            if geometry:\n",
        "                wkt = geojson_to_wkt(geometry)\n",
        "                \n",
        "                return (\n",
        "                    row.store_number,\n",
        "                    row.latitude,\n",
        "                    row.longitude,\n",
        "                    row.store_type,\n",
        "                    row.city,\n",
        "                    row.state,\n",
        "                    row.urbanicity_category,\n",
        "                    float(row.urbanicity_score),\n",
        "                    # float(row.population_density),\n",
        "                    int(row.drive_time_minutes),\n",
        "                    wkt\n",
        "                )\n",
        "    except Exception as e:\n",
        "        return None"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "isochrone_schema = StructType([\n",
        "    StructField(\"store_number\", StringType(), False),\n",
        "    StructField(\"latitude\", DoubleType(), False),\n",
        "    StructField(\"longitude\", DoubleType(), False),\n",
        "    StructField(\"store_type\", StringType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"state\", StringType(), True),\n",
        "    StructField(\"urbanicity_category\", StringType(), True),\n",
        "    StructField(\"urbanicity_score\", DoubleType(), True),\n",
        "    # StructField(\"population_density\", DoubleType(), True),\n",
        "    StructField(\"drive_time_minutes\", IntegerType(), False),\n",
        "    StructField(\"geometry_wkt\", StringType(), False)\n",
        "])\n",
        "\n",
        "location_rows = locations_with_drive_time.collect()\n",
        "\n",
        "results = []\n",
        "for i, row in enumerate(location_rows):\n",
        "    if i % 100 == 0:\n",
        "        print(f\"{i}/{len(location_rows)}\")\n",
        "    result = generate_isochrone(row)\n",
        "    if result:\n",
        "        results.append(result)\n",
        "\n",
        "isochrones = spark.createDataFrame(results, schema=isochrone_schema)\n",
        "generated_count = len(results)\n",
        "\n",
        "print(f\"{generated_count} isochrones generated\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md\n",
        "## Write to Silver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import current_timestamp\n",
        "\n",
        "isochrones_final = (\n",
        "    isochrones\n",
        "    .withColumn(\"geometry\", expr(\"ST_GeomFromText(geometry_wkt, 4326)\"))\n",
        "    .withColumn(\"area_sqkm\", expr(\"ST_Area(geometry) / 1000000\"))\n",
        "    .withColumn(\"created_timestamp\", current_timestamp())\n",
        "    .drop(\"geometry_wkt\")\n",
        "    .select(\n",
        "        \"store_number\",\n",
        "        \"latitude\",\n",
        "        \"longitude\",\n",
        "        \"store_type\",\n",
        "        \"city\",\n",
        "        \"state\",\n",
        "        \"urbanicity_category\",\n",
        "        \"urbanicity_score\",\n",
        "        \"drive_time_minutes\",\n",
        "        \"geometry\",\n",
        "        \"area_sqkm\",\n",
        "        \"created_timestamp\"\n",
        "    )\n",
        ")\n",
        "\n",
        "full_table_name = f\"{catalog}.{silver_schema}.silver_{output_table}\"\n",
        "write_mode = output_config['write_mode']\n",
        "\n",
        "(\n",
        "    isochrones_final\n",
        "    .write\n",
        "    .format(\"delta\")\n",
        "    .mode(write_mode)\n",
        "    .option(\"overwriteSchema\", \"true\")\n",
        "    .saveAsTable(full_table_name)\n",
        ")\n",
        "\n",
        "print(f\"Written {generated_count} isochrones to {full_table_name}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(spark.sql(f\"\"\"\n",
        "  SELECT\n",
        "    urbanicity_category,\n",
        "    drive_time_minutes,\n",
        "    COUNT(*) as count,\n",
        "    ROUND(AVG(area_sqkm), 2) as avg_area_sqkm\n",
        "  FROM {full_table_name}\n",
        "  GROUP BY urbanicity_category, drive_time_minutes\n",
        "  ORDER BY urbanicity_category, drive_time_minutes\n",
        "\"\"\"))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install folium"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import folium"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pyspark.sql.functions as F\n",
        "h3_sample = isochrones_final.limit(200)\n",
        "\n",
        "\n",
        "h3_geojson = h3_sample.withColumn(\n",
        "    \"geojson\",\n",
        "    F.expr(\"ST_AsGeoJSON(geometry)\")\n",
        ").select(\"store_number\", \"geojson\").collect()\n",
        "\n",
        "features = [\n",
        "    {\n",
        "        \"type\": \"Feature\",\n",
        "        # \"properties\": {\"h3_cell_id\": row[\"h3_cell_id\"]},\n",
        "        \"geometry\": json.loads(row[\"geojson\"])\n",
        "    }\n",
        "    for row in h3_geojson\n",
        "]\n",
        "\n",
        "m = folium.Map(location=[ 42.40, -71.38], zoom_start=6)\n",
        "folium.GeoJson(\n",
        "    {\"type\": \"FeatureCollection\", \"features\": features},\n",
        "    style_function=lambda x: {\"fillColor\": \"blue\", \"color\": \"black\", \"weight\": 1, \"fillOpacity\": 0.3}\n",
        ").add_to(m)\n",
        "\n",
        "m"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}