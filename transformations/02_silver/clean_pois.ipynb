{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "# MAGIC %md\n",
        "# MAGIC # POI Cleaning - Silver Layer\n",
        "# MAGIC\n",
        "# MAGIC Cleans and structures raw POI data from Bronze layer.\n",
        "# MAGIC\n",
        "# MAGIC **Purpose**: Transform raw OSM POI data into cleaned structured format.\n",
        "# MAGIC\n",
        "# MAGIC **Input**: Bronze POI table (`{catalog}.bronze.osm_pois_raw`)\n",
        "# MAGIC **Output**: Silver table with cleaned POI data (poi_id, name, category, latitude, longitude, address)\n",
        "# MAGIC"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import yaml\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime\n",
        "\n",
        "# Notebook parameters\n",
        "dbutils.widgets.text(\"catalog\", \"\")\n",
        "dbutils.widgets.text(\"bronze_schema\", \"\")\n",
        "dbutils.widgets.text(\"silver_schema\", \"\")\n",
        "dbutils.widgets.text(\"config_path\", \"\")\n",
        "dbutils.widgets.text(\"input_table\", \"\")\n",
        "dbutils.widgets.text(\"output_table\", \"\")\n",
        "\n",
        "# Extract parameters\n",
        "catalog = dbutils.widgets.get(\"catalog\")\n",
        "bronze_schema = dbutils.widgets.get(\"bronze_schema\")\n",
        "silver_schema = dbutils.widgets.get(\"silver_schema\")\n",
        "config_path = dbutils.widgets.get(\"config_path\")\n",
        "input_table_widget = dbutils.widgets.get(\"input_table\")\n",
        "output_table_widget = dbutils.widgets.get(\"output_table\")\n",
        "\n",
        "assert catalog and bronze_schema and silver_schema and config_path, \"Missing required parameters\"\n",
        "\n",
        "# Load configuration\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "poi_cleaning_config = config['poi_cleaning']\n",
        "table_config = config['table_names']\n",
        "\n",
        "# Use explicit tables if provided, otherwise construct from config\n",
        "if input_table_widget and input_table_widget.strip():\n",
        "    input_table = input_table_widget.strip()\n",
        "else:\n",
        "    input_table = f\"{catalog}.{bronze_schema}.bronze_{table_config['bronze_raw_suffix']}\"\n",
        "\n",
        "if output_table_widget and output_table_widget.strip():\n",
        "    output_table = output_table_widget.strip()\n",
        "else:\n",
        "    output_table = f\"{catalog}.{silver_schema}.silver_{table_config['silver_cleaned_suffix']}\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read raw POI data\n",
        "pois_raw = spark.read.table(input_table)\n",
        "\n",
        "# Validate table exists and has data\n",
        "poi_count = pois_raw.count()\n",
        "\n",
        "# Diagnostic: Show table info\n",
        "table_info = spark.createDataFrame([\n",
        "    (\"Input table\", input_table),\n",
        "    (\"Row count\", str(poi_count))\n",
        "], [\"info\", \"value\"])\n",
        "display(table_info)\n",
        "\n",
        "if poi_count == 0:\n",
        "    raise RuntimeError(f\"No POIs found in input table: {input_table}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md\n",
        "## Extract and Clean Columns\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract category and subcategory from tags using UDF - more reliable for Python dict handling\n",
        "category_priority = poi_cleaning_config.get('category_priority', ['shop', 'amenity', 'leisure', 'tourism', 'office', 'public_transport', 'railway'])\n",
        "\n",
        "def get_category_and_subcategory(tags):\n",
        "    \"\"\"Extract category and subcategory from tags dict\n",
        "    Returns (category, subcategory) tuple\n",
        "    Example: {'amenity': 'pharmacy'} -> ('amenity', 'pharmacy')\n",
        "    \"\"\"\n",
        "    if tags is None or not isinstance(tags, dict):\n",
        "        return (None, None)\n",
        "    \n",
        "    # Check each priority tag in order\n",
        "    for category_tag in category_priority:\n",
        "        if category_tag in tags:\n",
        "            tag_value = tags[category_tag]\n",
        "            if tag_value and str(tag_value).strip():\n",
        "                return (category_tag, str(tag_value).strip())\n",
        "    \n",
        "    return (None, None)\n",
        "\n",
        "# Create UDF with struct return type\n",
        "category_schema = StructType([\n",
        "    StructField(\"category\", StringType(), True),\n",
        "    StructField(\"subcategory\", StringType(), True)\n",
        "])\n",
        "get_category_udf = F.udf(get_category_and_subcategory, category_schema)\n",
        "\n",
        "# Extract address components using configured address fields\n",
        "address_fields = poi_cleaning_config.get('address_fields', ['addr:housenumber', 'addr:street', 'addr:city', 'addr:state', 'addr:postcode'])\n",
        "\n",
        "def build_address(tags):\n",
        "    \"\"\"Build address string from tags using configured address fields\"\"\"\n",
        "    if tags is None or not isinstance(tags, dict):\n",
        "        return None\n",
        "    \n",
        "    parts = []\n",
        "    for field in address_fields:\n",
        "        if field in tags and tags[field]:\n",
        "            parts.append(str(tags[field]))\n",
        "    \n",
        "    return ', '.join(parts) if parts else None\n",
        "\n",
        "build_address_udf = F.udf(build_address, StringType())\n",
        "\n",
        "# Clean POI data\n",
        "poi_id_prefix = poi_cleaning_config.get('poi_id_prefix', 'poi_')\n",
        "pois_with_category = pois_raw \\\n",
        "    .withColumn(\"poi_id\", F.concat(F.lit(poi_id_prefix), F.col(\"osm_id\"))) \\\n",
        "    .withColumn(\"name\", F.col(\"tags\")[\"name\"]) \\\n",
        "    .withColumn(\"category_struct\", get_category_udf(F.col(\"tags\"))) \\\n",
        "    .withColumn(\"poi_category\", F.col(\"category_struct.category\")) \\\n",
        "    .withColumn(\"poi_subcategory\", F.col(\"category_struct.subcategory\")) \\\n",
        "    .withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\")) \\\n",
        "    .withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\")) \\\n",
        "    .withColumn(\"address\", build_address_udf(F.col(\"tags\"))) \\\n",
        "    .withColumn(\"ingestion_timestamp\", F.lit(datetime.now()))\n",
        "\n",
        "# Apply filters and select final columns\n",
        "pois_cleaned = pois_with_category \\\n",
        "    .select(\n",
        "        \"poi_id\",\n",
        "        \"name\",\n",
        "        \"poi_category\",\n",
        "        \"poi_subcategory\",\n",
        "        \"latitude\",\n",
        "        \"longitude\",\n",
        "        \"address\",\n",
        "        \"osm_id\",\n",
        "        \"osm_type\",\n",
        "        \"ingestion_timestamp\"\n",
        "    ) \\\n",
        "    .filter(\n",
        "        F.col(\"latitude\").isNotNull() &\n",
        "        F.col(\"longitude\").isNotNull() &\n",
        "        F.col(\"poi_category\").isNotNull()\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md\n",
        "## Validate Data Quality\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Validate coordinate bounds using configured bounds\n",
        "coord_bounds = poi_cleaning_config.get('coordinate_bounds', {\n",
        "    'latitude_min': -90,\n",
        "    'latitude_max': 90,\n",
        "    'longitude_min': -180,\n",
        "    'longitude_max': 180\n",
        "})\n",
        "\n",
        "pois_cleaned = pois_cleaned.filter(\n",
        "    (F.col(\"latitude\") >= coord_bounds['latitude_min']) & \n",
        "    (F.col(\"latitude\") <= coord_bounds['latitude_max']) &\n",
        "    (F.col(\"longitude\") >= coord_bounds['longitude_min']) & \n",
        "    (F.col(\"longitude\") <= coord_bounds['longitude_max'])\n",
        ")\n",
        "\n",
        "display(pois_cleaned.limit(10))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md\n",
        "## Write to Silver Table\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write to Silver table\n",
        "pois_cleaned.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .option(\"delta.autoOptimize.optimizeWrite\", \"true\") \\\n",
        "    .option(\"delta.autoOptimize.autoCompact\", \"true\") \\\n",
        "    .saveAsTable(output_table)\n",
        "\n",
        "# Summary statistics\n",
        "summary = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_pois,\n",
        "        COUNT(DISTINCT poi_category) as poi_categories,\n",
        "        COUNT(DISTINCT poi_subcategory) as poi_subcategories,\n",
        "        COUNT(DISTINCT osm_type) as osm_types,\n",
        "        COUNT(CASE WHEN name IS NOT NULL THEN 1 END) as pois_with_name,\n",
        "        COUNT(CASE WHEN address IS NOT NULL THEN 1 END) as pois_with_address\n",
        "    FROM {output_table}\n",
        "\"\"\")\n",
        "\n",
        "display(summary)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}