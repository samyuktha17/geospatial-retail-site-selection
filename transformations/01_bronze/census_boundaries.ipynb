{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "# MAGIC %md\n",
        "# MAGIC # Census Boundaries - Bronze Layer Ingestion\n",
        "# MAGIC\n",
        "# MAGIC Ingests Census TIGER/Line cartographic boundary files into Unity Catalog using `pygris`.\n",
        "# MAGIC\n",
        "# MAGIC **Data Source:** Census Cartographic Boundary Files via `pygris` (500k resolution)\n",
        "# MAGIC\n",
        "# MAGIC **Geographies:**\n",
        "# MAGIC - Block Groups (by state)\n",
        "# MAGIC - States (All US)\n",
        "# MAGIC\n",
        "# MAGIC **Output Tables:**\n",
        "# MAGIC - `{catalog}.{bronze_schema}.bronze_census_blockgroups` - Block group boundaries with native GEOGRAPHY type\n",
        "# MAGIC - `{catalog}.{bronze_schema}.bronze_census_states` - State boundaries with native GEOGRAPHY type\n",
        "# MAGIC\n",
        "# MAGIC **Optimizations:**\n",
        "# MAGIC - Uses native Databricks GEOGRAPHY type (SRID 4326)\n",
        "# MAGIC - Direct WKT conversion from GeoPandas (most efficient path)\n",
        "# MAGIC - Uses `ST_GeomFromText()` for optimal performance\n",
        "# MAGIC - No intermediate GeoJSON conversions\n",
        "# MAGIC - Proper geometry validation and metadata"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pygris\n",
        "from pygris import states, block_groups\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "import geopandas as gpd\n",
        "\n",
        "# Notebook parameters\n",
        "dbutils.widgets.text(\"catalog\", \"\")\n",
        "dbutils.widgets.text(\"bronze_schema\", \"\")\n",
        "dbutils.widgets.text(\"boundary_data_volume\", \"\")\n",
        "dbutils.widgets.text(\"state_fips\", \"\")\n",
        "dbutils.widgets.text(\"year\", \"\")\n",
        "\n",
        "# Extract parameters\n",
        "catalog = dbutils.widgets.get(\"catalog\")\n",
        "bronze_schema = dbutils.widgets.get(\"bronze_schema\")\n",
        "boundary_data_volume = dbutils.widgets.get(\"boundary_data_volume\")\n",
        "state_fips = dbutils.widgets.get(\"state_fips\")\n",
        "year = int(dbutils.widgets.get(\"year\")) if dbutils.widgets.get(\"year\") else 2020\n",
        "\n",
        "assert catalog and bronze_schema, \"Missing required parameters\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def geopandas_to_spark_with_geometry(gdf, geography_level, ingest_id, ingest_timestamp):\n",
        "    \"\"\"\n",
        "    Convert GeoPandas GeoDataFrame to Spark DataFrame with native GEOGRAPHY type.\n",
        "    Optimized for Databricks using WKT format (most efficient conversion path).\n",
        "    \n",
        "    Args:\n",
        "        gdf: GeoPandas GeoDataFrame from pygris\n",
        "        geography_level: 'block_group' or 'state'\n",
        "        ingest_id: UUID for tracking ingestion batch\n",
        "        ingest_timestamp: Timestamp of ingestion\n",
        "    \n",
        "    Returns:\n",
        "        Spark DataFrame with native GEOGRAPHY column (SRID 4326)\n",
        "    \"\"\"\n",
        "    # Convert geometry to WKT strings (most efficient format for Databricks ST functions)\n",
        "    # WKT is simpler and faster than GeoJSON for ST_GeomFromText\n",
        "    gdf_copy = gdf.copy()\n",
        "    gdf_copy['geometry_wkt'] = gdf_copy['geometry'].apply(lambda geom: geom.wkt if geom is not None else None)\n",
        "    gdf_copy = gdf_copy.drop(columns=['geometry'])\n",
        "    \n",
        "    # Create Spark DataFrame from pandas\n",
        "    spark_df = spark.createDataFrame(gdf_copy)\n",
        "    \n",
        "    # Convert WKT to native GEOGRAPHY type with explicit SRID 4326 (WGS 84)\n",
        "    # IMPORTANT: Explicitly specify SRID to ensure consistency across all polygons\n",
        "    spark_df = spark_df.withColumn(\n",
        "        \"geometry\",\n",
        "        F.expr(\"ST_GeomFromText(geometry_wkt, 4326)\")\n",
        "    ).drop(\"geometry_wkt\")\n",
        "    \n",
        "    # Add ingestion metadata\n",
        "    spark_df = (spark_df\n",
        "                .withColumn(\"geography_level\", F.lit(geography_level))\n",
        "                .withColumn(\"ingestion_id\", F.lit(ingest_id))\n",
        "                .withColumn(\"ingestion_timestamp\", F.lit(ingest_timestamp)))\n",
        "    \n",
        "    return spark_df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate ingestion metadata\n",
        "ingest_id = str(uuid.uuid4())\n",
        "ingest_timestamp = datetime.now()\n",
        "\n",
        "# Fetch ALL Block Groups for specified state using pygris\n",
        "# cb=True gets cartographic boundary files (simplified for mapping)\n",
        "bg_gdf = block_groups(\n",
        "    state=state_fips,\n",
        "    county=None,  # Get all counties in the state\n",
        "    year=year,\n",
        "    cache=True,\n",
        "    cb=True  # Cartographic boundaries (500k resolution)\n",
        ")\n",
        "\n",
        "# Fetch all US states with cartographic boundaries\n",
        "states_gdf = states(\n",
        "    cb=True,\n",
        "    resolution='500k',\n",
        "    year=year,\n",
        "    # cache=True\n",
        ")\n",
        "\n",
        "# Convert GeoPandas GeoDataFrames to Spark DataFrames with geometry\n",
        "bg_df = geopandas_to_spark_with_geometry(bg_gdf, \"block_group\", ingest_id, ingest_timestamp)\n",
        "state_df = geopandas_to_spark_with_geometry(states_gdf, \"state\", ingest_id, ingest_timestamp)\n",
        "\n",
        "# Standardize block group columns (uppercase to match pygris schema)\n",
        "bg_df = (bg_df\n",
        "         .withColumnRenamed(\"GEOID\", \"geoid\")\n",
        "         .withColumnRenamed(\"NAME\", \"name\")\n",
        "         .withColumnRenamed(\"STATEFP\", \"state_fips\")\n",
        "         .withColumnRenamed(\"COUNTYFP\", \"county_fips\")\n",
        "         .withColumnRenamed(\"TRACTCE\", \"tract\")\n",
        "         .withColumnRenamed(\"BLKGRPCE\", \"block_group_id\")\n",
        "         .withColumnRenamed(\"ALAND\", \"area_land\")\n",
        "         .withColumnRenamed(\"AWATER\", \"area_water\"))\n",
        "\n",
        "# Standardize state columns\n",
        "state_df = (state_df\n",
        "            .withColumnRenamed(\"GEOID\", \"geoid\")\n",
        "            .withColumnRenamed(\"STUSPS\", \"state_abbr\")\n",
        "            .withColumnRenamed(\"NAME\", \"name\")\n",
        "            .withColumnRenamed(\"STATEFP\", \"state_fips\")\n",
        "            .withColumnRenamed(\"ALAND\", \"area_land\")\n",
        "            .withColumnRenamed(\"AWATER\", \"area_water\"))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Spark best practice: Optimize write operations\n",
        "# Use repartition based on data size and cluster configuration\n",
        "bg_table = f\"{catalog}.{bronze_schema}.bronze_census_blockgroups\"\n",
        "states_table = f\"{catalog}.{bronze_schema}.bronze_census_states\"\n",
        "\n",
        "(bg_df\n",
        " .repartition(10)  # Optimize based on data size\n",
        " .write\n",
        " .mode(\"overwrite\")\n",
        " .option(\"mergeSchema\", \"true\")\n",
        " .option(\"overwriteSchema\", \"true\")\n",
        " .saveAsTable(bg_table))\n",
        "\n",
        "(state_df\n",
        " .repartition(1)  # Small dataset, single partition sufficient\n",
        " .write\n",
        " .mode(\"overwrite\")\n",
        " .option(\"mergeSchema\", \"true\")\n",
        " .option(\"overwriteSchema\", \"true\")\n",
        " .saveAsTable(states_table))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%md\n",
        "## Validation: Verify GEOGRAPHY Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verify that geometry columns are saved with native GEOGRAPHY type and SRID 4326\n",
        "print(\"=\" * 80)\n",
        "print(\"GEOMETRY TYPE AND SRID VALIDATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check block groups table\n",
        "print(f\"\\n1. Block Groups Table ({bg_table}):\")\n",
        "bg_validation = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_rows,\n",
        "        COUNT(geometry) as non_null_geometries,\n",
        "        TYPEOF(geometry) as geometry_type,\n",
        "        ST_SRID(FIRST(geometry)) as srid\n",
        "    FROM {bg_table}\n",
        "\"\"\")\n",
        "bg_validation.show(truncate=False)\n",
        "\n",
        "# Check for empty geometries\n",
        "bg_empty_check = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as empty_geometry_count\n",
        "    FROM {bg_table}\n",
        "    WHERE ST_IsEmpty(geometry) = true\n",
        "\"\"\")\n",
        "print(\"Empty geometry check:\")\n",
        "bg_empty_check.show(truncate=False)\n",
        "\n",
        "# Sample a few geometries to ensure they're valid\n",
        "print(\"\\nSample block group geometries:\")\n",
        "spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        geoid,\n",
        "        name,\n",
        "        state_fips,\n",
        "        ST_GeometryType(geometry) as geom_type,\n",
        "        ST_SRID(geometry) as srid,\n",
        "        ST_Area(geometry) as area_sqm\n",
        "    FROM {bg_table}\n",
        "    LIMIT 3\n",
        "\"\"\").show(truncate=False)\n",
        "\n",
        "# Check states table\n",
        "print(f\"\\n2. States Table ({states_table}):\")\n",
        "state_validation = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as total_rows,\n",
        "        COUNT(geometry) as non_null_geometries,\n",
        "        TYPEOF(geometry) as geometry_type,\n",
        "        ST_SRID(FIRST(geometry)) as srid\n",
        "    FROM {states_table}\n",
        "\"\"\")\n",
        "state_validation.show(truncate=False)\n",
        "\n",
        "# Check for empty geometries in states\n",
        "state_empty_check = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        COUNT(*) as empty_geometry_count\n",
        "    FROM {states_table}\n",
        "    WHERE ST_IsEmpty(geometry) = true\n",
        "\"\"\")\n",
        "print(\"Empty geometry check:\")\n",
        "state_empty_check.show(truncate=False)\n",
        "\n",
        "# Sample a state geometry\n",
        "print(\"\\nSample state geometry:\")\n",
        "spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        state_abbr,\n",
        "        name,\n",
        "        ST_GeometryType(geometry) as geom_type,\n",
        "        ST_SRID(geometry) as srid,\n",
        "        ST_Area(geometry) / 1000000 as area_sqkm\n",
        "    FROM {states_table}\n",
        "    WHERE state_fips = '{state_fips}'\n",
        "\"\"\").show(truncate=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\u2713 VALIDATION COMPLETE\")\n",
        "print(\"  - Verify geometry_type = GEOGRAPHY\")\n",
        "print(\"  - Verify SRID = 4326 (WGS 84)\")\n",
        "print(\"  - Verify empty_geometry_count = 0\")\n",
        "print(\"=\" * 80)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}