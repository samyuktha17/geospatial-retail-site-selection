{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "# MAGIC %md\n",
        "# MAGIC # OSM Road Network Download - Bronze Layer\n",
        "# MAGIC\n",
        "# MAGIC Downloads OpenStreetMap road network data from Geofabrik and stores in Unity Catalog.\n",
        "# MAGIC\n",
        "# MAGIC **Data Source:** Geofabrik OSM Extracts  \n",
        "# MAGIC **Format:** PBF (Protocolbuffer Binary Format)  \n",
        "# MAGIC **Region:** Massachusetts\n",
        "# MAGIC\n",
        "# MAGIC **Output:**\n",
        "# MAGIC - Volume: `/Volumes/{catalog}/bronze/osm_data/massachusetts-latest.osm.pbf`\n",
        "# MAGIC - Table: `{catalog}.{bronze_schema}.bronze_osm_downloads` (tracking metadata)\n",
        "# MAGIC"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "# Notebook parameters\n",
        "dbutils.widgets.text(\"catalog\", \"\")\n",
        "dbutils.widgets.text(\"bronze_schema\", \"\")\n",
        "dbutils.widgets.text(\"osm_data_volume\", \"\")\n",
        "dbutils.widgets.text(\"osm_url\", \"\")\n",
        "dbutils.widgets.text(\"region\", \"\")\n",
        "\n",
        "# Extract parameters\n",
        "catalog = dbutils.widgets.get(\"catalog\")\n",
        "bronze_schema = dbutils.widgets.get(\"bronze_schema\")\n",
        "osm_data_volume = dbutils.widgets.get(\"osm_data_volume\")\n",
        "osm_url = dbutils.widgets.get(\"osm_url\")\n",
        "region = dbutils.widgets.get(\"region\")\n",
        "\n",
        "assert catalog and bronze_schema and osm_url, \"Missing required parameters\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil\n",
        "# Download OSM file\n",
        "download_id = str(uuid.uuid4())\n",
        "download_start = datetime.now()\n",
        "\n",
        "# Extract filename from URL\n",
        "osm_filename = osm_url.split('/')[-1]\n",
        "volume_file_path = f\"{osm_data_volume}{osm_filename}\"\n",
        "\n",
        "# Check if file already exists (idempotency)\n",
        "try:\n",
        "    existing_files = dbutils.fs.ls(volume_file_path)\n",
        "    file_size_mb = existing_files[0].size / (1024 * 1024)\n",
        "    status = \"existing\"\n",
        "    download_end = download_start\n",
        "except:\n",
        "    # File doesn't exist, download it\n",
        "    status = \"downloading\"\n",
        "    \n",
        "    # Create volume if needed\n",
        "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{bronze_schema}.bronze_osm_data\")\n",
        "    \n",
        "    # Download to local temp location\n",
        "    temp_path = f\"/tmp/{osm_filename}\"\n",
        "    \n",
        "    with requests.get(osm_url, stream=True, timeout=600) as response:\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        file_size_bytes = int(response.headers.get('content-length', 0))\n",
        "        file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "        \n",
        "        with open(temp_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192*1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "    \n",
        "    # Copy to volume using dbutils\n",
        "    # Direct download to volume (no temp file)\n",
        "\n",
        "    with requests.get(osm_url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(volume_file_path.replace(\"dbfs:\", \"/dbfs\"), \"wb\") as f:\n",
        "            shutil.copyfileobj(r.raw, f)\n",
        "        \n",
        "        # Cleanup\n",
        "        import os\n",
        "        os.remove(temp_path)\n",
        "        \n",
        "        download_end = datetime.now()\n",
        "        status = \"completed\"\n",
        "\n",
        "    duration_seconds = (download_end - download_start).total_seconds()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "status"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write tracking metadata to Unity Catalog\n",
        "osm_table = f\"{catalog}.{bronze_schema}.bronze_osm_downloads\"\n",
        "\n",
        "data = [(\n",
        "    download_id,\n",
        "    datetime.now().date(),\n",
        "    region,\n",
        "    volume_file_path,\n",
        "    int(file_size_mb),\n",
        "    status,\n",
        "    duration_seconds,\n",
        "    download_start,\n",
        "    download_end\n",
        ")]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"download_id\", StringType(), False),\n",
        "    StructField(\"download_date\", DateType(), False),\n",
        "    StructField(\"region\", StringType(), False),\n",
        "    StructField(\"osm_file_path\", StringType(), False),\n",
        "    StructField(\"file_size_mb\", LongType(), False),\n",
        "    StructField(\"download_status\", StringType(), False),\n",
        "    StructField(\"duration_seconds\", DoubleType(), False),\n",
        "    StructField(\"download_start\", TimestampType(), False),\n",
        "    StructField(\"download_end\", TimestampType(), False)\n",
        "])\n",
        "\n",
        "osm_df = spark.createDataFrame(data, schema)\n",
        "\n",
        "(osm_df\n",
        " .write\n",
        " .mode(\"append\")\n",
        " .saveAsTable(osm_table))"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}